{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440dbbd1",
   "metadata": {},
   "source": [
    "# LIDAR Camera object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba12c6",
   "metadata": {},
   "source": [
    "### Use get_frame() to get the depth and color frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd1c66",
   "metadata": {},
   "source": [
    "get_frame() returns arg1, arg2, arg3\n",
    "arg1 = is valid (boolean)\n",
    "arg2 = depth frame (np array)\n",
    "arg3 = color frame (np array - resized to depth frame dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a72d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "class LIDAR_Object:\n",
    "    def __init__(self):\n",
    "        # Configure depth and color streams\n",
    "        self.pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "    \n",
    "        # Get device product line for setting a supporting resolution\n",
    "        pipeline_wrapper = rs.pipeline_wrapper(self.pipeline)\n",
    "        pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "        device = pipeline_profile.get_device()\n",
    "        device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "    \n",
    "        found_rgb = False\n",
    "        for s in device.sensors:\n",
    "            if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "                found_rgb = True\n",
    "                break\n",
    "        if not found_rgb:\n",
    "            print(\"The demo requires Depth camera with Color sensor\")\n",
    "            exit(0)\n",
    "    \n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        \n",
    "        if device_product_line == 'L500':\n",
    "            config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, 30)\n",
    "        else:\n",
    "            config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "        # Start streaming\n",
    "        self.pipeline.start(config)\n",
    "        \n",
    "    def get_frame(self):\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv.applyColorMap(cv.convertScaleAbs(depth_image, alpha=0.03), cv.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "        \n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            color_image = cv.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv.INTER_AREA)\n",
    "        #    images = np.hstack((color_image, depth_colormap))\n",
    "        #else:\n",
    "        #    images = np.hstack((color_image, depth_colormap))\n",
    "        \n",
    "        if not depth_frame or not color_frame:\n",
    "            return False, None, None\n",
    "        return True, depth_image, color_image\n",
    "\n",
    "    def release(self):\n",
    "        self.pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11dd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = LIDAR_Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b5eee1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n",
      "Click\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "counter = 0\n",
    "def save_frame(event, x, y, args, params):\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        global counter\n",
    "        global img\n",
    "        for i in range(10):\n",
    "            counter+=1\n",
    "            print('Click')\n",
    "            cv.imwrite(str(counter)+'.jpg', img)\n",
    "\n",
    "# Create mouse event\n",
    "cv.namedWindow(\"Color frame\")\n",
    "cv.setMouseCallback(\"Color frame\", save_frame)\n",
    "\n",
    "while True:\n",
    "    ret, depth_frame, color_frame = cam.get_frame()\n",
    "    img = color_frame\n",
    "    cv.imshow(\"depth frame\", depth_frame)\n",
    "    cv.imshow(\"Color frame\", color_frame)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a5012",
   "metadata": {},
   "source": [
    "### Mouse cursor distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c691dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "point = (400, 300)\n",
    "\n",
    "def show_distance(event, x, y, args, params):\n",
    "    global point\n",
    "    point = (x, y)\n",
    "\n",
    "# Create mouse event\n",
    "cv.namedWindow(\"Color frame\")\n",
    "cv.setMouseCallback(\"Color frame\", show_distance)\n",
    "\n",
    "while True:\n",
    "    ret, depth_frame, color_frame = cam.get_frame()\n",
    "\n",
    "    # Show distance for a specific point\n",
    "    cv.circle(color_frame, point, 4, (0, 0, 255))\n",
    "    distance = depth_frame[point[1], point[0]]\n",
    "\n",
    "    cv.putText(color_frame, \"{}mm\".format(distance), (point[0], point[1] - 20), cv.FONT_HERSHEY_PLAIN, 2, (0, 0, 0), 2)\n",
    "\n",
    "    cv.imshow(\"depth frame\", depth_frame)\n",
    "    cv.imshow(\"Color frame\", color_frame)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250319d",
   "metadata": {},
   "source": [
    "### Hand tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0b5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Define currently used camera\n",
    "USEDCAMERA = 0\n",
    "# Define camera width and height\n",
    "CAMWIDTH = 640\n",
    "CAMHEIGHT = 480\n",
    "# Define text color\n",
    "TEXTCOLOR = (100, 150, 210)\n",
    "\n",
    "\n",
    "# The hand detector model class.\n",
    "# Initialize with empty parenthesis or pass the desired values in the object initialization.\n",
    "# The arguments passed are the arguments needed to create the mediapipe Hands model\n",
    "class HandDetector:\n",
    "    def __init__(self,\n",
    "                 static_image_mode=False,\n",
    "                 max_num_hands=2,\n",
    "                 model_complexity=1,\n",
    "                 min_detection_confidence=0.5,\n",
    "                 min_tracking_confidence=0.5):\n",
    "        self.results = None\n",
    "        self.mode = static_image_mode\n",
    "        self.max_hands = max_num_hands\n",
    "        self.complexity = model_complexity\n",
    "        self.minimal_detection_confidence = min_detection_confidence\n",
    "        self.minimal_tracking_confidence = min_tracking_confidence\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(self.mode, self.max_hands, self.complexity,\n",
    "                                         self.minimal_detection_confidence,\n",
    "                                         self.minimal_tracking_confidence)  # Create a mediapipe Hands class object.\n",
    "        self.mp_draw_utility = mp.solutions.drawing_utils  # Create a mediapipe drawing utility to draw the points\n",
    "\n",
    "    # find_hands() takes a frame, activates mediapipe's hand tracking algorithm and returns the frame with the hands\n",
    "    # marked (points and connecting lines). As default the 'draw' argument is set to 'True'. Set to 'False' to cancel\n",
    "    # drawing the points and lines on the frame.\n",
    "    def find_hands(self, frame, draw=True):\n",
    "        rgb_frame = cv.cvtColor(frame,  # Convert the image to RGB. mediapipe uses RGB images\n",
    "                                cv.COLOR_BGR2RGB)  # and open-cv uses BGR images.\n",
    "        self.results = self.hands.process(rgb_frame)  # Process the frame using the Hands.process() method\n",
    "        if self.results.multi_hand_landmarks:  # If True, a hand or more were found. Else equals to None\n",
    "            for hand in self.results.multi_hand_landmarks:  # Loop through the found hands and perform actions on each\n",
    "                if draw:\n",
    "                    self.mp_draw_utility.draw_landmarks(frame, hand,  # Draw the 21 points\n",
    "                                                        self.mp_hands.HAND_CONNECTIONS)  # and the connecting lines\n",
    "                    # on each hand\n",
    "        return frame\n",
    "\n",
    "    # find_position() takes a frame and the hand number to be processed (set as default to '0' - the first hand in\n",
    "    # the list), finds the position of each point and returns that list\n",
    "    def find_landmarks(self, frame, hand_number=USEDCAMERA):\n",
    "        landmarks = []\n",
    "        if self.results.multi_hand_landmarks:  # If True, a hand or more were found. Else equals to None\n",
    "            hand = self.results.multi_hand_landmarks[hand_number]\n",
    "            for index, landmark in enumerate(hand.landmark):  # Match the index of the landmark to the landmark\n",
    "                frame_height, frame_width, channels = frame.shape\n",
    "                center_x, center_y = int(landmark.x * frame_width), int(landmark.y * frame_height)\n",
    "                landmarks.append([index, center_x, center_y])\n",
    "        return landmarks\n",
    "\n",
    "    def run(self, camera):\n",
    "        while True:\n",
    "            ret, depth_frame, color_frame = cam.get_frame()\n",
    "            frame = self.find_hands(color_frame)\n",
    "            self.find_landmarks(frame)\n",
    "            if ret:  # If a frame exists\n",
    "                cv.imshow('Camera', frame)  # Display the current frame (BGR frame)\n",
    "                if cv.waitKey(1) & 0xFF == ord('q'):  # Press 'q' key to stop and exit\n",
    "                    break\n",
    "            else:  # No frame exists, break the loop\n",
    "                break\n",
    "        cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130461df",
   "metadata": {},
   "outputs": [],
   "source": [
    "htm = HandDetector()\n",
    "cam = LIDAR_Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b4dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "htm.run(cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8029d14b",
   "metadata": {},
   "source": [
    "### Pose/Skeleton recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9bcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5, enable_segmentation=True) as pose:\n",
    "    while True:\n",
    "        success, depth, image = cam.get_frame()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Draw the pose annotation on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv.imshow('MediaPipe Pose', cv.flip(image, 1))\n",
    "        if cv.waitKey(5) & 0xFF == 27:\n",
    "            cv.destroyAllWindows()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd061554",
   "metadata": {},
   "source": [
    "# Save images / Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4180d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = LIDAR_Object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95006771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "counter = 0\n",
    "def save_frame(event, x, y, args, params):\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        global counter\n",
    "        global img\n",
    "        counter+=1\n",
    "        cv.imwrite(str(counter)+'.jpg', img)\n",
    "\n",
    "# Create mouse event\n",
    "cv.namedWindow(\"Color frame\")\n",
    "cv.setMouseCallback(\"Color frame\", save_frame)\n",
    "\n",
    "while True:\n",
    "    ret, depth_frame, color_frame = cam.get_frame()\n",
    "    img = color_frame\n",
    "    cv.imshow(\"depth frame\", depth_frame)\n",
    "    cv.imshow(\"Color frame\", color_frame)\n",
    "    key = cv.waitKey(1)\n",
    "    if key == 27:\n",
    "        cv.destroyAllWindows()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ecf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
